{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:\n",
      "1758815\n",
      "Number of words:\n",
      "736456\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re\n",
    "\n",
    "cwd = 'C:/Users/klouc/Desktop/slovcho/spell_checking'\n",
    "\n",
    "sents = pd.read_csv(rf\"{cwd}/sents.csv\",encoding='utf-8')\n",
    "valid_words = pd.read_csv(rf\"{cwd}/single_words_bg.csv\",encoding='utf-8')\n",
    "\n",
    "valid_words_dict = {}\n",
    "for word in list(valid_words['word']):\n",
    "    valid_words_dict[word] = word\n",
    "\n",
    "print(\"Number of sentences:\")\n",
    "print(len(sents.values))\n",
    "print(\"Number of words:\")\n",
    "print(len(valid_words.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [<s>, по, какво, четеш, Квантова, физика, Ама,...\n",
       "1    [<s>, на, тила, му, е, сцената, от, древногръц...\n",
       "2    [<s>, декември, г, Европейският, съвет, в, Люк...\n",
       "3    [<s>, АД, е, различно, дружество, от, Лукойл, ...\n",
       "4    [<s>, АД, от, агенция, Митници, С, решение, на...\n",
       "5    [<s>, Абе, вече, забравихме, руският, украинск...\n",
       "6    [<s>, в, тази, насока, препоръчвам, филмчето, ...\n",
       "7    [<s>, вярвате, ли, че, халифата, ще, приключи,...\n",
       "8    [<s>, Габи, ще, си, оправи, ли, леглото, не, о...\n",
       "9    [<s>, Европа, е, най, западният, полуостров, н...\n",
       "Name: sent, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process sents\n",
    "# Sents vocab АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЬЮЯабвгдежзийклмнопрстуфхцчшщъьюя1234567890!@#$%^&*()-_=+`~[]{}|;':\"\\,./<>\n",
    "\n",
    "# TODO: Break sentences up more as some 'sents' include multiple sentences\n",
    "# TODO: Lowers first letter of sent but what if first word has multiple capitalised letters\n",
    "# TODO: Tokenise sentences in a more clever way\n",
    "\n",
    "def process_sent(sentence):\n",
    "\n",
    "    replace_with_space_chars = '[!@#$%^&*()-_=+`~{}\\|;\\[\\]\\':\\\"\\,./<>]'\n",
    "\n",
    "    # CLEAN SENTENCES\n",
    "    # Replace some chars with space\n",
    "    sentence = re.sub(replace_with_space_chars, ' ', sentence)\n",
    "    # Replace consecutive spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # If sent below 10 characters do not consider\n",
    "    if(len(sentence) < 5): return np.nan\n",
    "\n",
    "    # Remove first charatcer if space\n",
    "    if(sentence[0] == ' '): sentence = sentence[1:]\n",
    "    # Lower letter if non-capitlised word exists in dict\n",
    "    first_word = ''\n",
    "    if(sentence.find(' ')):\n",
    "        first_word = sentence[:sentence.find(' ')]\n",
    "    else:\n",
    "        first_word = sentence\n",
    "    if(ord(first_word[0]) >= ord('А') and ord(first_word[0]) <= ord('Я') and (first_word[0].lower() + first_word[1:]) in valid_words_dict):\n",
    "        sentence = sentence[0].lower() + sentence[1:]\n",
    "\n",
    "    # ADD BEGINNING AND CLOSING TAG\n",
    "    sentence = f'<s> {sentence} </s>'\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "cleaned_sents = sents[\"sent\"].apply(lambda x : process_sent(x))\n",
    "# Drop rows with no content\n",
    "cleaned_sents.dropna(inplace=True)\n",
    "cleaned_sents.reset_index(inplace=True, drop=True)\n",
    "\n",
    "tokenised_sents = cleaned_sents.apply(lambda x : x.split(' '))\n",
    "tokenised_sents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unigrams and bigrams\n",
    "\n",
    "unigrams = nltk.FreqDist()\n",
    "bigrams = []\n",
    "\n",
    "for tokenised_sent in tokenised_sents:\n",
    "    unigrams.update(nltk.FreqDist(tokenised_sent))\n",
    "    bigrams.extend(nltk.bigrams(tokenised_sent))\n",
    "\n",
    "# pairs = []\n",
    "# for trigrams in trigrams:\n",
    "#     pair = ((trigrams[0], trigrams[1]), trigrams[2])\n",
    "#     pairs.append(pair)\n",
    "# trigrams = nltk.ConditionalFreqDist(pairs)\n",
    "bigrams = nltk.ConditionalFreqDist(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# model = pipeline(\n",
    "#     'ner',\n",
    "#     model='rmihaylov/bert-base-ner-theseus-bg',\n",
    "#     tokenizer='rmihaylov/bert-base-ner-theseus-bg',\n",
    "#     device=0,\n",
    "#     revision=None)\n",
    "# output = model('И така де отидохме до Ню Йорк')\n",
    "# print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenise input in clever way\n",
    "# TODO: Dealing with non-cyrillic characters\n",
    "# TODO: How to deal if first word (capitalised but only cause start of sent) is wrong? As in, discerning capital letter\n",
    "\n",
    "import re\n",
    "\n",
    "sent_to_check = 'Аз обичам много да седя на червеното стол'\n",
    "\n",
    "# PRE-PROCESS INPUT\n",
    "\n",
    "# Clear punctuation\n",
    "sent_to_check = sent_to_check\n",
    "sent_to_check = re.sub(\"[?!.,:]\", \"\", sent_to_check)\n",
    "\n",
    "# Tokenise\n",
    "words_to_check = sent_to_check.split(' ')\n",
    "\n",
    "# Lower first character\n",
    "if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "     words_to_check[0] = words_to_check[0].lower()\n",
    "# # If beginning word is capitalised and a non-capitalised version exists in the dict, lower it (might be a name)\n",
    "# # Else do not lower\n",
    "# if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "#     if(words_to_check[0].lower() in valid_words.values):\n",
    "#         words_to_check[0] = words_to_check[0].lower()\n",
    "\n",
    "# Add <s> and </s>\n",
    "words_to_check.insert(0, '<s>')\n",
    "words_to_check.append('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('аз', 2747370514.34881), ('за', 7901.343871326475), ('а', 174.4627777603499), ('из', 0.19931628486404976), ('газ', 0.017804640621022987), ('ай', 0.011797220751847458), ('ах', 0.005002666189276102), ('таз', 0.0020606666460922015), ('ад', 0.0014027847966484501), ('ар', 0.00049731209615791), ('ас', 0.0003552109247467033), ('раз', 0.00013430772339872733), ('яз', 0.0001082507812841248), ('ат', 1.3508187323814917e-11), ('юз', 9.558649471792044e-21)]\n",
      "[('обичам', 24631.621212125403), ('обича', 0.03923413341093485), ('обичат', 0.00786848018303382), ('обичаи', 0.0010129847542196862), ('обичаш', 0.0003491226830766992), ('обичан', 0.00024861766828431956), ('обичал', 0.0001401781599131489), ('обичах', 8.407398027664347e-05), ('обичай', 5.282057339313957e-10), ('обичаме', 3.7622724856082005e-10), ('обичая', 1.6404453986518545e-10), ('обичащ', 4.687402186302246e-11), ('обличам', 2.4067286446863754e-11), ('обирам', 2.1307095028013938e-12), ('обричам', 5.88299672550733e-20), ('опичам', 5.729034001286064e-20), ('осичам', 5.729034001286064e-20), ('отичам', 5.729034001286064e-20)]\n",
      "[('много', 64895510.04248853)]\n",
      "[('да', 4186647233.596817), ('а', 441119.40237578686), ('на', 140.37206631978992), ('за', 127.15093398142997), ('са', 74.04383011442192), ('до', 2.992381365285793), ('два', 0.1474399432065272), ('та', 0.04257698226480775), ('де', 0.011084563086832313), ('ма', 0.005152285311173456), ('дал', 0.0012451515181216746), ('па', 0.0011052515014006552), ('дай', 1.2702015604276275e-09), ('ха', 1.0548692832628923e-09), ('дам', 8.999329795134364e-10), ('дар', 3.2182006306820065e-10), ('ад', 2.886478573871078e-10), ('ада', 2.711037799888993e-10), ('ида', 1.2367990071309922e-10), ('ба', 9.380427844267796e-11), ('ща', 7.187808989675685e-11), ('ду', 5.604171839288916e-11), ('яда', 2.69585250772545e-11), ('ода', 2.5372770366629762e-11), ('юда', 6.3433152651461925e-12), ('дра', 3.1716678742492887e-12), ('дуа', 1.5858365135459835e-12)]\n",
      "[('седи', 2034703.8626210943), ('седят', 749029.7149720461), ('седя', 5679.101621808363), ('сея', 3.41667482004136), ('следя', 0.006243134225349278), ('съдя', 0.002004655717952791), ('седял', 0.00032173790201896484), ('сетя', 5.6348093592000974e-05), ('седящ', 2.992911925148948e-05), ('редя', 2.817400906128506e-05), ('седях', 1.6835129601069152e-05), ('садя', 2.4570835559509625e-06), ('педя', 2.4587455740124846e-11), ('цедя', 9.457094907999354e-13), ('уседя', 2.7012207382313528e-20), ('селя', 2.5428158183967977e-20), ('бедя', 2.5428158183967977e-20)]\n",
      "[('на', 311844697.7764835), ('за', 0.9530344890004073), ('са', 0.49363630899233657), ('да', 0.27417267645717375), ('а', 0.049351647044807684), ('нас', 0.011475619937433986), ('не', 2.711433656881617e-07), ('но', 1.2590876211546984e-07), ('най', 1.1781544882895874e-07), ('над', 4.591912431431466e-08), ('ни', 3.348203002305499e-08), ('та', 1.1432257057646143e-08), ('нар', 5.924197272130453e-09), ('ма', 2.766858186666171e-09), ('ха', 1.1798634605876922e-09), ('наш', 1.0791543549520763e-09), ('па', 8.903059262505098e-10), ('ба', 1.0491938891136235e-10), ('нам', 8.868031469561093e-11), ('ща', 8.039510983171314e-11), ('нае', 5.1436322768322345e-11)]\n",
      "[('червеното', 0.007410924376856297), ('черненото', 1.7316576276662624e-20), ('черпеното', 1.7316576276662624e-20), ('червенето', 1.7316576276662624e-20)]\n",
      "[('стил', 0.003520067821341992), ('стои', 4.139724657483873e-09), ('сто', 3.8613862583495715e-09), ('сол', 9.196496153417672e-10), ('стол', 5.570053596292769e-10), ('стола', 3.6567727458175286e-10), ('стоп', 3.363219960606986e-10), ('стой', 2.710503692909639e-10), ('стоя', 2.453353988441582e-10), ('стоял', 2.354019844536725e-10), ('ствол', 2.0808289748786084e-10), ('стон', 3.166168890107443e-11), ('атол', 1.780990089292131e-11)]\n"
     ]
    }
   ],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдежзийклмнопрстуфхцчшщъьюя'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "unigram_sum = sum(unigrams.values())\n",
    "unigram_count = len(unigrams.keys())\n",
    "trust_valid_word_is_correct = 0.95\n",
    "\n",
    "for i in range(len(words_to_check)):\n",
    "    word = words_to_check[i]\n",
    "    if word != '<s>' and word != '</s>':\n",
    "        prev_token = words_to_check[i - 1]\n",
    "        next_token = words_to_check[i + 1]\n",
    "        candidates = edits1(word)\n",
    "\n",
    "        # Get candidates which are valid words\n",
    "        valid_candidates = []\n",
    "        for candidate in candidates:\n",
    "            if(candidate in valid_words.values):\n",
    "                valid_candidates.append(candidate)\n",
    "\n",
    "        # Non-word errors\n",
    "        if word not in valid_words.values:\n",
    "            # Calculate probabilty distribution\n",
    "            prob_distr = []\n",
    "            for valid_candidate in valid_candidates:\n",
    "                # [MISSING] = P(prob of making mistake)\n",
    "\n",
    "                # P(candidate) with smooting = probability of word appering\n",
    "                # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                # P(candidate|prev_token) with smoothing = probability of findging candidate followed by prev_token\n",
    "                # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                prob_distr.append(pc * pcp * pnc)\n",
    "            words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "            words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "            print(words_with_probs)\n",
    "\n",
    "        # Real world errors\n",
    "        else:\n",
    "            # Calculate probabilty distribution\n",
    "            prob_distr = []\n",
    "            # valid_candidates.append(word)\n",
    "            for valid_candidate in valid_candidates:\n",
    "                # [MISSING] = P(prob of making mistake)\n",
    "\n",
    "                # P(candidate) with smooting = probability of word appering\n",
    "                # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                # P(candidate|prev_token) with smoothing = probability of findging candidate followed by prev_token\n",
    "                # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                trust = 0\n",
    "                # Apply trust for valid word\n",
    "                if(word == valid_candidate):\n",
    "                    trust = trust_valid_word_is_correct\n",
    "                else:\n",
    "                    trust = 1 - trust_valid_word_is_correct / len(valid_candidate)\n",
    "                prob_distr.append(pc * pcp * pnc * trust)\n",
    "            words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "            words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "            print(words_with_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
