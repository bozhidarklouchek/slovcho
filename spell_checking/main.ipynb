{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:\n",
      "1758815\n",
      "Number of words:\n",
      "736456\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re\n",
    "\n",
    "cwd = 'C:/Users/klouc/Desktop/slovcho/spell_checking'\n",
    "\n",
    "sents = pd.read_csv(rf\"{cwd}/sents.csv\",encoding='utf-8')\n",
    "valid_words = pd.read_csv(rf\"{cwd}/single_words_bg.csv\",encoding='utf-8')\n",
    "\n",
    "valid_words_dict = {}\n",
    "for word in list(valid_words['word']):\n",
    "    valid_words_dict[word] = word\n",
    "\n",
    "print(\"Number of sentences:\")\n",
    "print(len(sents.values))\n",
    "print(\"Number of words:\")\n",
    "print(len(valid_words.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [<s>, по, какво, четеш, Квантова, физика, Ама,...\n",
       "1    [<s>, на, тила, му, е, сцената, от, древногръц...\n",
       "2    [<s>, декември, г, Европейският, съвет, в, Люк...\n",
       "3    [<s>, АД, е, различно, дружество, от, Лукойл, ...\n",
       "4    [<s>, АД, от, агенция, Митници, С, решение, на...\n",
       "5    [<s>, Абе, вече, забравихме, руският, украинск...\n",
       "6    [<s>, в, тази, насока, препоръчвам, филмчето, ...\n",
       "7    [<s>, вярвате, ли, че, халифата, ще, приключи,...\n",
       "8    [<s>, Габи, ще, си, оправи, ли, леглото, не, о...\n",
       "9    [<s>, Европа, е, най, западният, полуостров, н...\n",
       "Name: sent, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process sents\n",
    "# Sents vocab АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЬЮЯабвгдежзийклмнопрстуфхцчшщъьюя1234567890!@#$%^&*()-_=+`~[]{}|;':\"\\,./<>\n",
    "\n",
    "# TODO: Break sentences up more as some 'sents' include multiple sentences\n",
    "# TODO: Lowers first letter of sent but what if first word has multiple capitalised letters\n",
    "# TODO: Tokenise sentences in a more clever way\n",
    "\n",
    "def process_sent(sentence):\n",
    "\n",
    "    replace_with_space_chars = '[!@#$%^&*()-_=+`~{}\\|;\\[\\]\\':\\\"\\,./<>]'\n",
    "\n",
    "    # CLEAN SENTENCES\n",
    "    # Replace some chars with space\n",
    "    sentence = re.sub(replace_with_space_chars, ' ', sentence)\n",
    "    # Replace consecutive spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # If sent below 10 characters do not consider\n",
    "    if(len(sentence) < 5): return np.nan\n",
    "\n",
    "    # Remove first charatcer if space\n",
    "    if(sentence[0] == ' '): sentence = sentence[1:]\n",
    "    # Lower letter if non-capitlised word exists in dict\n",
    "    first_word = ''\n",
    "    if(sentence.find(' ')):\n",
    "        first_word = sentence[:sentence.find(' ')]\n",
    "    else:\n",
    "        first_word = sentence\n",
    "    if(ord(first_word[0]) >= ord('А') and ord(first_word[0]) <= ord('Я') and (first_word[0].lower() + first_word[1:]) in valid_words_dict):\n",
    "        sentence = sentence[0].lower() + sentence[1:]\n",
    "\n",
    "    # ADD BEGINNING AND CLOSING TAG\n",
    "    sentence = f'<s> {sentence} </s>'\n",
    "\n",
    "    return sentence\n",
    "    \n",
    "cleaned_sents = sents[\"sent\"].apply(lambda x : process_sent(x))\n",
    "# Drop rows with no content\n",
    "cleaned_sents.dropna(inplace=True)\n",
    "cleaned_sents.reset_index(inplace=True, drop=True)\n",
    "\n",
    "tokenised_sents = cleaned_sents.apply(lambda x : x.split(' '))\n",
    "tokenised_sents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unigrams and bigrams\n",
    "\n",
    "unigrams = nltk.FreqDist()\n",
    "bigrams = []\n",
    "\n",
    "for tokenised_sent in tokenised_sents:\n",
    "    unigrams.update(nltk.FreqDist(tokenised_sent))\n",
    "    bigrams.extend(nltk.trigrams(tokenised_sent))\n",
    "\n",
    "bigrams = nltk.ConditionalFreqDist(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenise input in clever way\n",
    "# TODO: Dealing with non-cyrillic characters\n",
    "# TODO: How to deal if first word (capitalised but only cause start of sent) is wrong? As in, discerning capital letter\n",
    "\n",
    "import re\n",
    "\n",
    "sent_to_check = 'той обича да играе с неговата копка'\n",
    "\n",
    "# PRE-PROCESS INPUT\n",
    "\n",
    "# Clear punctuation\n",
    "sent_to_check = sent_to_check\n",
    "sent_to_check = re.sub(\"[?!.,:]\", \"\", sent_to_check)\n",
    "\n",
    "# Tokenise\n",
    "words_to_check = sent_to_check.split(' ')\n",
    "\n",
    "# Lower first character\n",
    "if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "     words_to_check[0] = words_to_check[0].lower()\n",
    "# # If beginning word is capitalised and a non-capitalised version exists in the dict, lower it (might be a name)\n",
    "# # Else do not lower\n",
    "# if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "#     if(words_to_check[0].lower() in valid_words.values):\n",
    "#         words_to_check[0] = words_to_check[0].lower()\n",
    "\n",
    "# Add <s> and </s>\n",
    "words_to_check.insert(0, '<s>')\n",
    "words_to_check.append('</s>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('той', 108969293193.83888), ('кой', 49530756.711005874), ('то', 89.84784705398206), ('тъй', 29.55516694564533), ('тоя', 3.2994693843514007), ('том', 0.05035934456456159), ('бой', 0.03630500200295024), ('топ', 0.03537061389426542), ('ток', 0.022001525423675075), ('мой', 0.016556561360464267), ('стой', 0.005889572229933068), ('тон', 0.0031671801105727483), ('тор', 0.00209582757745562), ('тай', 0.0006517089714673821), ('рой', 0.00039304047912952543), ('вой', 0.00029059986621005393), ('твой', 0.0002124791092182442), ('пой', 2.6423277424972485e-05), ('дой', 8.807872709330106e-06), ('ой', 8.458766553309802e-06), ('сой', 8.328624467136766e-12), ('лой', 6.015166131127083e-12), ('гой', 2.3135552629981814e-12)]\n",
      "[('обича', 47147243.48175534), ('обичал', 14720.754954256308), ('облича', 901.4266372831771), ('обичат', 0.3280397610255443), ('обичам', 0.07225827384472802), ('обичай', 0.008100882758406834), ('обичаш', 0.004631149802103834), ('обичая', 0.001024708526527512), ('обида', 0.0009151336949602707), ('обичаи', 0.00046335766236322174), ('обичах', 0.0004125449914176291), ('обира', 0.00029750314171468785), ('обичащ', 0.00013307905292725666), ('обрича', 0.00013065949683659832), ('обичта', 7.016904556213269e-05), ('обич', 2.6658704271164115e-10), ('обичан', 1.8320195349007055e-10), ('обица', 3.751706967046645e-11), ('бича', 1.76587818325697e-11), ('отича', 1.3131249451552477e-11), ('обуча', 3.751815818018924e-12), ('обична', 1.949249039368085e-12), ('обичи', 1.875910956698791e-12), ('опича', 1.875910956698791e-12), ('обкича', 5.241124615453916e-20), ('обичка', 5.241124615453916e-20), ('осича', 5.0439337883378283e-20), ('окича', 5.0439337883378283e-20)]\n",
      "[('да', 419520591388.3514), ('на', 14432034.177461391), ('до', 1409770.6741898938), ('а', 343092.5348862278), ('за', 1.4988326249757642), ('де', 0.0036948585274093546), ('са', 2.698829343181062e-07), ('два', 2.6332840862148962e-08), ('та', 1.140640166025543e-08), ('ма', 2.76060061061804e-09), ('дал', 2.0014590805326504e-09), ('дай', 1.4174979082023052e-09), ('ха', 1.1771950602458825e-09), ('дам', 1.004291882268749e-09), ('па', 8.882923944162934e-10), ('дар', 3.5913927397721867e-10), ('ад', 3.221203207430757e-10), ('ада', 3.0254177999169484e-10), ('ида', 1.3802218955585668e-10), ('ба', 1.0468210134158365e-10), ('ща', 8.021328681089689e-11), ('ду', 6.254048260438209e-11), ('яда', 3.0084715761459616e-11), ('ода', 2.8315072221991935e-11), ('юда', 7.078904954569057e-12), ('дра', 3.5394639066159284e-12), ('дуа', 1.7697348284987784e-12)]\n",
      "[('играе', 2144314294.8049116), ('играем', 161212.84837941884), ('играя', 137648.17831222116), ('играеш', 42891.34092775851), ('игра', 0.20707891251564878), ('играч', 0.030884965363616533), ('играл', 0.0051752411661270484), ('играещ', 0.00018011617682965082), ('играй', 3.491730111370721e-05), ('играх', 1.870570455729204e-05), ('играех', 5.183203526070953e-06), ('игране', 6.479004560137248e-07), ('играел', 3.4446854033056264e-11)]\n",
      "[('с', 18281671598.014496), ('в', 14125691589.7276), ('и', 7430725887.849792), ('е', 36132510.41624815), ('се', 27867666.028663225), ('са', 5974936.277731427), ('а', 1501029.7696493473), ('си', 463531.7764463329), ('у', 0.000675081274679964), ('я', 2.4207645844229434e-09), ('ос', 5.765729304998206e-10), ('ся', 3.8536815047227915e-10), ('о', 3.029919585601624e-10), ('ас', 1.4152256698827434e-10)]\n",
      "[('неговата', 1.4878554791396137)]\n",
      "[('капка', 0.00029744877856987975), ('топка', 1.1791124845479287e-09), ('котка', 5.877417000246078e-10), ('копка', 2.102865724162182e-10), ('кока', 1.079477793630334e-10), ('клопка', 4.116518707104934e-11), ('копа', 3.729321402971301e-11), ('копна', 1.6680884233525967e-11), ('копки', 8.340495897139093e-12), ('койка', 4.170261414817824e-12), ('комка', 4.170261414817824e-12), ('корка', 2.0851340950115625e-12), ('ковка', 2.0851340950115625e-12), ('шопка', 5.606491223630486e-20), ('чопка', 5.606491223630486e-20)]\n"
     ]
    }
   ],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдежзийклмнопрстуфхцчшщъьюя'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "unigram_sum = sum(unigrams.values())\n",
    "unigram_count = len(unigrams.keys())\n",
    "trust_valid_word_is_correct = 0.95\n",
    "\n",
    "for i in range(len(words_to_check)):\n",
    "    word = words_to_check[i]\n",
    "    if word != '<s>' and word != '</s>':\n",
    "        prev_token = words_to_check[i - 1]\n",
    "        next_token = words_to_check[i + 1]\n",
    "        candidates = edits1(word)\n",
    "\n",
    "        # Get candidates which are valid words\n",
    "        valid_candidates = []\n",
    "        for candidate in candidates:\n",
    "            if(candidate in valid_words.values):\n",
    "                valid_candidates.append(candidate)\n",
    "\n",
    "        # Non-word errors\n",
    "        if word not in valid_words.values:\n",
    "            # Calculate probabilty distribution\n",
    "            prob_distr = []\n",
    "            for valid_candidate in valid_candidates:\n",
    "                # [MISSING] = P(prob of making mistake)\n",
    "\n",
    "                # P(candidate) with smooting = probability of word appering\n",
    "                # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                # P(candidate|prev_token) with smoothing = probability of findging candidate followed by prev_token\n",
    "                # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                prob_distr.append(pc * pcp * pnc)\n",
    "            words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "            words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "            print(words_with_probs)\n",
    "\n",
    "        # Real world errors\n",
    "        else:\n",
    "            # Calculate probabilty distribution\n",
    "            prob_distr = []\n",
    "            # valid_candidates.append(word)\n",
    "            for valid_candidate in valid_candidates:\n",
    "                # [MISSING] = P(prob of making mistake)\n",
    "\n",
    "                # P(candidate) with smooting = probability of word appering\n",
    "                # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                # P(candidate|prev_token) with smoothing = probability of findging candidate followed by prev_token\n",
    "                # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                trust = 0\n",
    "                # Apply trust for valid word\n",
    "                if(word == valid_candidate):\n",
    "                    trust = trust_valid_word_is_correct\n",
    "                else:\n",
    "                    trust = 1 - trust_valid_word_is_correct / len(valid_candidate)\n",
    "                prob_distr.append(pc * pcp * pnc * trust)\n",
    "            words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "            words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "            print(words_with_probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
