{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences:\n",
      "2221299\n",
      "Number of words:\n",
      "1147599\n"
     ]
    }
   ],
   "source": [
    "cwd = 'C:/Users/klouc/Desktop/slovcho/src/backend/spell_checking'\n",
    "\n",
    "sents = pd.read_csv(rf\"{cwd}/sents1.csv\",encoding='utf-8')\n",
    "valid_words = pd.read_csv(rf\"{cwd}/single_words_bg.csv\",encoding='utf-8')\n",
    "\n",
    "valid_words_dict = defaultdict(int)\n",
    "for word in list(valid_words['word']):\n",
    "    # print(word)\n",
    "    valid_words_dict[word] = word\n",
    "\n",
    "print(\"Number of sentences:\")\n",
    "print(len(sents.values))\n",
    "print(\"Number of words:\")\n",
    "print(len(valid_words.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500000\n",
      "1000000\n",
      "1500000\n",
      "2000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      [[да, Tx], [живее, Vpitf-r3s], [Кралят, Ncmsf]]\n",
       "1    [[негов, Hmsi], [водещ, Vpitcar-smi], [бе, Vxi...\n",
       "2    [[Маджурова.Светът, Ncmsf], [стана, Vppif-o3s]...\n",
       "3    [[Чарлз, Npmsi], [III, Momsi], [беше, Vxitf-t3...\n",
       "4    [[той, Ppe-os3m], [наследи, Vpptf-o3s], [трона...\n",
       "5      [[Да, Tx], [живее, Vpitf-r3s], [Кралят, Ncmsf]]\n",
       "6    [[дигитално, Ansi], [студио, Ncnsi], [със, R],...\n",
       "7    [[тя, Ppe-os3f], [беше, Vxitf-t3s], [извършена...\n",
       "8    [[те, Ppe-op3], [са, Vxitf-r3p], [получили, Vp...\n",
       "9    [[III?Чарлз, Momsi], [III, Momsi], [е, Vxitf-r...\n",
       "Name: sent, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pre-process sents\n",
    "# Sents vocab АБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЬЮЯабвгдежзийклмнопрстуфхцчшщъьюя1234567890!@#$%^&*()-_=+`~[]{}|;':\"\\,./<>\n",
    "\n",
    "# TODO: Lowers first letter of sent but what if first word has multiple capitalised letters\n",
    "c = 0\n",
    "def process_sent(sentence):\n",
    "\n",
    "    sentence = sentence.split('_____')\n",
    "    tagged_sent = [word_pos.split('///') for word_pos in sentence]\n",
    "\n",
    "    # replace_with_space_chars = '[!@#$%^&*()-_=+`~{}\\|;\\[\\]\\':\\\"\\,./<>]'\n",
    "\n",
    "    # CLEAN SENTENCES\n",
    "    # Replace some chars with space\n",
    "    # sentence = re.sub(replace_with_space_chars, ' ', sentence)\n",
    "    # # Replace consecutive spaces with single space\n",
    "    # sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # Remove punctuation\n",
    "    tagged_sent = [word_pos for word_pos in tagged_sent if(word_pos[1] != 'punct')]\n",
    "\n",
    "    # If sent below 3 tokens do not consider\n",
    "    if(len(tagged_sent) < 3): return np.nan\n",
    "    \n",
    "    # Clean empty words\n",
    "    tagged_sent = [word_pos for word_pos in tagged_sent if(word_pos[0] != '')]\n",
    "\n",
    "    # # Remove first charatcer if space\n",
    "    # if(sentence[0] == ' '): sentence = sentence[1:]\n",
    "    # Lower letter if non-capitlised word exists in dict\n",
    "    first_token = tagged_sent[0][0]\n",
    "    # if(sentence.find(' ')):\n",
    "    #     first_word = sentence[:sentence.find(' ')]\n",
    "    # else:\n",
    "    #     first_word = sentence\n",
    "    if(ord(first_token[0]) >= ord('А') and ord(first_token[0]) <= ord('Я') and (valid_words_dict[first_token[0].lower() + first_token[1:]] != 0)):\n",
    "        first_token = first_token[0].lower() + first_token[1:]\n",
    "        tagged_sent[0][0] = first_token\n",
    "\n",
    "    # Remove any escape char tokens\n",
    "    tagged_sent = [word_pos for word_pos in tagged_sent if(word_pos[0] != '\\n' and\n",
    "                                                           word_pos[0] != '\\r' and\n",
    "                                                           word_pos[0] != '\\t' and\n",
    "                                                           word_pos[0] != '\\b' and\n",
    "                                                           word_pos[0] != '\\f') and\n",
    "                                                           word_pos[0][0] != '\\\\']\n",
    "    # tagged_sent = \"_____\".join([\"///\".join(word_pos) for word_pos in tagged_sent])\n",
    "    # print(tagged_sent)\n",
    "    # # ADD BEGINNING AND CLOSING TAG\n",
    "    # tagged_sent = f'<s>_____{tagged_sent}_____</s>'\n",
    "    global c\n",
    "    if(c % 500000 == 0):\n",
    "        print(c)\n",
    "    c += 1\n",
    "    return tagged_sent\n",
    "    \n",
    "cleaned_sents = sents[\"sent\"].apply(lambda x : process_sent(x))\n",
    "\n",
    "# Drop rows with no content\n",
    "cleaned_sents.dropna(inplace=True)\n",
    "cleaned_sents.reset_index(inplace=True, drop=True)\n",
    "\n",
    "cleaned_sents.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2204144/2204144 [02:40<00:00, 13733.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method ConditionalFreqDist.conditions of <ConditionalFreqDist with 717835 conditions>>\n"
     ]
    }
   ],
   "source": [
    "# Create unigrams and bigrams\n",
    "import tqdm\n",
    "from random import shuffle\n",
    "\n",
    "unigrams = nltk.FreqDist()\n",
    "bigrams = []\n",
    "\n",
    "\n",
    "shuffle(cleaned_sents)\n",
    "\n",
    "for tokenised_sent in tqdm.tqdm(cleaned_sents):\n",
    "    tokens_only = [word_pos[0] for word_pos in tokenised_sent]\n",
    "    tokens_only.insert(0, '<s>')\n",
    "    tokens_only.append('</s>')\n",
    "    # print(tokens_only)\n",
    "\n",
    "    tokens_frq = nltk.FreqDist(tokens_only)\n",
    "    tokens_bigrams = nltk.bigrams(tokens_only)\n",
    "\n",
    "    unigrams.update(tokens_frq)\n",
    "    bigrams.extend(tokens_bigrams)\n",
    "\n",
    "# pairs = []\n",
    "# for trigrams in trigrams:\n",
    "#     pair = ((trigrams[0], trigrams[1]), trigrams[2])\n",
    "#     pairs.append(pair)\n",
    "# trigrams = nltk.ConditionalFreqDist(pairs)\n",
    "bigrams = nltk.ConditionalFreqDist(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save unigram and bigram objects to files\n",
    "with open('unigrams.pkl', 'wb') as file:\n",
    "    pickle.dump(unigrams, file)\n",
    "\n",
    "with open('bigrams.pkl', 'wb') as file:\n",
    "    pickle.dump(bigrams, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'B-LOC', 'score': 0.99997365, 'index': 7, 'word': '▁Франция', 'start': 29, 'end': 37}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model = pipeline(\n",
    "    'ner',\n",
    "    model='rmihaylov/bert-base-ner-theseus-bg',\n",
    "    tokenizer='rmihaylov/bert-base-ner-theseus-bg',\n",
    "    device=0,\n",
    "    revision=None)\n",
    "output = model('Бил ли си някога във голямата Франция?')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenise input in clever way\n",
    "# TODO: Dealing with non-cyrillic characters\n",
    "# TODO: How to deal if first word (capitalised but only cause start of sent) is wrong? As in, discerning capital letter\n",
    "\n",
    "  # PRE-PROCESS INPUT\n",
    "def preprocess_input(sent_to_check):\n",
    "  # Clear punctuation\n",
    "  sent_to_check = re.sub(\"[?!.,:]\", \"\", sent_to_check)\n",
    "\n",
    "  # Tokenise\n",
    "  words_to_check = sent_to_check.split(' ')\n",
    "\n",
    "  # Lower first character\n",
    "  if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "      words_to_check[0] = words_to_check[0].lower()\n",
    "  # # If beginning word is capitalised and a non-capitalised version exists in the dict, lower it (might be a name)\n",
    "  # # Else do not lower\n",
    "  # if(ord(words_to_check[0][0]) >= ord('А') and ord(words_to_check[0][0]) <= ord('Я')):\n",
    "  #     if(words_to_check[0].lower() in valid_words.values):\n",
    "  #         words_to_check[0] = words_to_check[0].lower()\n",
    "\n",
    "  # Add <s> and </s>\n",
    "  words_to_check.insert(0, '<s>')\n",
    "  words_to_check.append('</s>')\n",
    "  return words_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'абвгдежзийклмнопрстуфхцчшщъьюя'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def get_word_probabilities(words_to_check, unigram_sum, unigram_count, trust_valid_word_is_correct, bigrams, valid_words_dict, CORRECT_NON_WORDS_ONLY=True):\n",
    "  sent_probs = []\n",
    "  for i in range(len(words_to_check)):\n",
    "      word = words_to_check[i]\n",
    "      if word != '<s>' and word != '</s>':\n",
    "          prev_token = words_to_check[i - 1]\n",
    "          next_token = words_to_check[i + 1]\n",
    "          candidates = edits1(word)\n",
    "\n",
    "          # Get candidates which are valid words\n",
    "          valid_candidates = []\n",
    "          for candidate in candidates:\n",
    "              if(valid_words_dict[candidate] != 0):\n",
    "                  valid_candidates.append(candidate)\n",
    "\n",
    "          # Non-word errors\n",
    "          if word not in valid_words.values:\n",
    "              # Calculate probabilty distribution\n",
    "              prob_distr = []\n",
    "              for valid_candidate in valid_candidates:\n",
    "                  # [MISSING] P(prob of making mistake)\n",
    "\n",
    "                  # P(candidate) with smooting = probability of word appering\n",
    "                  # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                  pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                  # P(candidate|prev_token) with smoothing = probability of finding candidate followed by prev_token\n",
    "                  # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                  pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                  # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                  # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                  pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                  prob_distr.append(pc * pcp * pnc)\n",
    "              words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "              words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "              sent_probs.append(words_with_probs)\n",
    "\n",
    "          # Real world errors\n",
    "          else:\n",
    "              if(CORRECT_NON_WORDS_ONLY):\n",
    "                sent_probs.append([(word, 1)])\n",
    "                continue\n",
    "              # Calculate probabilty distribution\n",
    "              prob_distr = []\n",
    "              # valid_candidates.append(word)\n",
    "              for valid_candidate in valid_candidates:\n",
    "                  # [MISSING] = P(prob of making mistake)\n",
    "\n",
    "                  # P(candidate) with smooting = probability of word appering\n",
    "                  # candidate count in word unigram + 1 / count of all words + |V|\n",
    "                  pc = unigrams[valid_candidate] + 1 / float(unigram_sum + unigram_count)\n",
    "\n",
    "                  # P(candidate|prev_token) with smoothing = probability of findging candidate followed by prev_token\n",
    "                  # count of bigrams (prev_token, candidate) + 1 / count of prev_token + |V|\n",
    "                  pcp = bigrams[prev_token][valid_candidate] + 1 / float(unigrams[prev_token] + unigram_count)\n",
    "\n",
    "                  # P(next_token|candidate) with smoothing = probability of findging next_token followed by candidate\n",
    "                  # count of bigrams (candidate, next_token) + 1 / count of candidate + |V|\n",
    "                  pnc = bigrams[valid_candidate][next_token] + 1 / float(unigrams[valid_candidate] + unigram_count)\n",
    "\n",
    "                  trust = 0\n",
    "                  # Apply trust for valid word\n",
    "                  if(word == valid_candidate):\n",
    "                      trust = trust_valid_word_is_correct\n",
    "                  else:\n",
    "                      trust = 1 - trust_valid_word_is_correct / len(valid_candidate)\n",
    "                  prob_distr.append(pc * pcp * pnc * trust)\n",
    "              words_with_probs = [(word, prob) for word, prob in zip(valid_candidates, prob_distr)]\n",
    "              words_with_probs.sort(key = lambda x: x[1], reverse=True)\n",
    "              sent_probs.append(words_with_probs)\n",
    "  return sent_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load unigram and bigram objects from files\n",
    "with open('unigrams.pkl', 'rb') as file:\n",
    "    unigrams = pickle.load(file)\n",
    "\n",
    "with open('bigrams.pkl', 'rb') as file:\n",
    "    bigrams = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('удавникът', 2.7861449377466165e-06)]\n",
      "[('умря', 1)]\n"
     ]
    }
   ],
   "source": [
    "unigram_sum = sum(unigrams.values())\n",
    "unigram_count = len(unigrams.keys())\n",
    "trust_valid_word_is_correct = 0.99\n",
    "\n",
    "cwd = 'C:/Users/klouc/Desktop/slovcho/src/backend/spell_checking'\n",
    "valid_words = pd.read_csv(rf\"{cwd}/single_words_bg.csv\",encoding='utf-8')\n",
    "valid_words_dict = defaultdict(int)\n",
    "for word in list(valid_words['word']):\n",
    "    valid_words_dict[word] = word\n",
    "\n",
    "\n",
    "sent_to_check = 'одавникът умря'\n",
    "preprocessed_input = preprocess_input(sent_to_check)\n",
    "probs = get_word_probabilities(preprocessed_input, unigram_sum, unigram_count, trust_valid_word_is_correct, bigrams, valid_words_dict, CORRECT_NON_WORDS_ONLY=True)\n",
    "for prob in probs:\n",
    "  print(prob)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
